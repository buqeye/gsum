from __future__ import division
import pymc3 as pm
import numpy as np
import theano
import theano.tensor as tt
import warnings
from pymc3.math import cartesian
import scipy as sp
import scipy.integrate as integrate
import scipy.stats as st
from .extras import Exponential, coefficients, predictions, gaussian, stabilize
from statsmodels.sandbox.distributions.mv_normal import MVT
from functools import reduce


__all__ = ['GPCoeffs', 'InCoeffs', 'MvBLM', 'GeoProcess']


class CoeffBase(object):
    """A base class that sets up the observable coefficient model.

    This simply extracts and sets up the relevant variables to be used
    in the coefficient models.
    """

    def __init__(self, X, obs, orders, rm_orders=None, ref=1, Q_est=1,
                 grid=False, **kwargs):
        self.grid = grid
        if grid:
            self.Xs = X
            self.X = cartesian(*X)
        else:
            self.Xs = None
            self.X = X
        self.N = len(self.X)

        try:
            self.Q_est_func = Q_est
            self.Q_est = Q_est(self.X)
        except TypeError:
            self.Q_est = Q_est * np.ones(len(self.X))
            self.Q_est_func = None

        assert len(obs) == len(orders), \
            "Orders must match the number of coefficients"
        if rm_orders is None:
            rm_orders = []

        # self.orders = []
        # self.cs = []
        # for i, n in enumerate(orders):
        #     if i == 0:
        #         cn = obs[i] / (ref * self.Q_est**n)
        #     else:
        #         cn = (obs[i] - obs[i-1]) / (ref * self.Q_est**n)

        #     if n not in rm_orders:
        #         self.orders.append(n)
        #         self.cs.append(cn)
        # self.orders = np.array(self.orders)

        # Find differences but keep leading term
        cs = np.diff(obs)
        cs = np.insert(cs, 0, obs[0])

        # Make coefficients
        ordervec = np.atleast_2d(orders).T
        cs = cs / (ref * Q**ordervec)

        # Remove unwanted orders
        keep_indices = np.logical_not(np.isin(orders, rm_orders))
        self.orders = orders[keep_indices]
        self.cs = cs[keep_indices]

        # Get max order
        k_arg = np.argmax(self.orders)
        self.k = self.orders[k_arg]
        self.obsk = np.array(obs[k_arg])
        self.ref = ref


class MvBLM(object):
    """A multivariate Bayesian linear model class

    Implements a Bayesian linear model for an Nx1 vector Y::

        Y ~ N(H\beta, \sigma^2 R)

    with an Nxq matrix H, qx1 vector \beta, scalar \sigma^2, and NxN
    correlation matrix R. A normal-inverse-gamma prior is placed on \beta and
    \sigma^2::

        \beta, \sigma^2 ~ NIG(means, cov, shape, scale)

    The matrix H is generated by a basis function.
    """

    def __init__(self, dim=None, basis=None, means=None, cov=None, shape=None, scale=None, corr=None):
        # pm.Model.__init__(self, name=name, model=model)
        # CoeffBase.__init__(
        #     self, X=X, obs=obs, orders=orders, rm_orders=rm_orders,
        #     ref=ref, Q_est=Q_est, grid=grid)

        if corr is None:
            corr = gaussian
        if not callable(corr):
            raise ValueError('corr must be callable')
        self.corr = corr

        if basis == 0:
            raise ValueError('basis must be non-zero scalar or callable')
        elif basis is None:
            basis = 1
        self.basis = self._domain_function(basis, cols=1)
        # if not callable(basis):
        #     raise ValueError('basis must be callable')
        # self.basis = basis

        if dim is None:
            dim = self.basis(np.zeros((1, 1))).shape[1]

        if dim < 1:
            raise ValueError('dim must be greater than or equal to one')

        if means is None:
            means = np.zeros(dim)
        else:
            means = np.atleast_1d(means)

        if means.shape[0] != dim:
            raise ValueError('means length does not match dim')

        if means.ndim > 1:
            raise ValueError('means must be 1d array or scalar')

        self.means_0 = means
        # self.mean_dim = len(self.means_0)

        if cov is None:
            self.inv_cov_0 = np.zeros((dim, dim))
        else:
            self.inv_cov_0 = np.linalg.inv(cov)

        if self.inv_cov_0.shape != (dim, dim):
            raise ValueError('Shape of cov must be (dim, dim)')

        if shape is None:
            self.shape_0 = 0
        else:
            self.shape_0 = shape

        if scale is None:
            self.scale_0 = 0
        else:
            self.scale_0 = scale

        self._shape = self.shape_0
        self._scale = self.scale_0
        self._means = self.means_0
        self._cov = cov
        self._inv_cov = self.inv_cov_0

    def _domain_function(self, obj, cols=None):
        try:
            isNumber = 0 == 0*obj
        except:
            isNumber = False

        if callable(obj):
            return obj
        elif isNumber:
            def dom_func(X, **kwargs):
                if cols is None:
                    vec = np.ones(len(X))
                else:
                    vec = np.ones((len(X), cols))
                return obj * vec
            return dom_func
        else:
            raise ValueError('{} must be a number or function'.format(obj))

    def observe(self, X, y, **corr_kwargs):

        if X.ndim != 2 and y != 2:
            raise ValueError('X and y must be 2d arrays')

        if y.shape[-1] != X.shape[0]:
            raise ValueError('X row length must match y column length')

        self._X = X
        self._y = y

        self._corr_kwargs = corr_kwargs
        self._chol = np.linalg.cholesky(stabilize(self.corr(X, **corr_kwargs)))
        self._shape = self.shape(y=y)
        self._scale = self.scale(y=y, **corr_kwargs)
        self._means = self.means(y=y, **corr_kwargs)
        self._inv_cov = self.inv_cov(y=y, **corr_kwargs)
        self._cov = self.cov(y=y, **corr_kwargs)

    # @property
    # def shape(self):
    #     """The shape parameter a of the inverse gamma distribution"""
    #     return self._shape

    # def _update_shape(self):
    #     self._shape = self.shape_0 + self.N * self.num_y / 2.0

    def _recompute_corr(self, **corr_kwargs):
        # Must be non-empty and not equal to the defaults
        return corr_kwargs and corr_kwargs != self._corr_kwargs

    def shape(self, y=None):
        if y is None:
            return self._shape
        num_y, N = y.shape
        return self.shape_0 + N * num_y / 2.0

    # @property
    # def scale(self):
    #     """The scale parameter b of the inverse gamma distribution"""
    #     return self._scale

    # def _update_scale(self):
    #     means_0, inv_cov_0 = self.means_0, self.inv_cov_0
    #     means, inv_cov = self.means, self.inv_cov
    #     y, R_chol = self.y, self.R_chol
    #     val = np.dot(means_0.T, np.dot(inv_cov_0, means_0)) + \
    #         np.trace(np.dot(y, sp.linalg.cho_solve((R_chol, True), y.T))) - \
    #         np.dot(means.T, np.dot(inv_cov, means))
    #     self._scale = self.scale_0 + val / 2.0

    def scale(self, y=None, **corr_kwargs):
        if y is None and not self._recompute_corr(**corr_kwargs):
            return self._scale

        # Set up variables
        means_0, inv_cov_0 = self.means_0, self.inv_cov_0
        means = self.means(y=y, **corr_kwargs)
        inv_cov = self.inv_cov(y=y, **corr_kwargs)
        y = y if y is not None else self.y
        # if corr_kwargs:  # not empty dict
        #     R_chol = np.cholesky(stabilize(self.corr(self.X, **corr_kwargs)))
        # else:
        #     R_chol = self.R_chol
        R_chol = self.chol(**corr_kwargs)

        # Compute quadratics
        val = np.dot(means_0.T, np.dot(inv_cov_0, means_0)) + \
            np.trace(np.dot(y, sp.linalg.cho_solve((R_chol, True), y.T))) - \
            np.dot(means.T, np.dot(inv_cov, means))
        return self.scale_0 + val / 2.0

    # @property
    # def means(self):
    #     return self._means

    # def _update_means(self):
    #     Rinv_y = sp.linalg.cho_solve((self.R_chol, True), self.avg_y)
    #     val = np.dot(self.inv_cov_0, self.means_0) + \
    #         self.num_y * np.dot(self.H.T, Rinv_y)
    #     self._means = np.dot(self.cov, val)

    def means(self, y=None, **corr_kwargs):
        if y is None and not self._recompute_corr(**corr_kwargs):
            return self._means
        y = y if y is not None else self.y
        num_y = y.shape[0]
        avg_y = np.average(y, axis=0)
        R_chol = self.chol(**corr_kwargs)
        H = self.basis(self.X)
        cov = self.cov(y=y, **corr_kwargs)

        Rinv_y = sp.linalg.cho_solve((R_chol, True), avg_y)
        val = np.dot(self.inv_cov_0, self.means_0) + \
            num_y * np.dot(H.T, Rinv_y)
        return np.dot(cov, val)

    # @property
    # def cov(self):
    #     return self._cov

    # def _update_cov(self):
    #     right = sp.linalg.solve_triangular(self.R_chol, self.H, lower=True)
    #     quad = np.dot(right.T, right)
    #     self._inv_cov = self.inv_cov_0 + self.num_y * quad
    #     self._cov = np.linalg.inv(self.inv_cov)

    def inv_cov(self, y=None, **corr_kwargs):
        if y is None and not self._recompute_corr(**corr_kwargs):
            return self._inv_cov
        y = y if y is not None else self.y
        num_y = y.shape[0]
        # num_y = num_y if num_y is not None else self.num_y
        R_chol = self.chol(**corr_kwargs)
        H = self.basis(self.X)

        right = sp.linalg.solve_triangular(R_chol, H, lower=True)
        quad = np.dot(right.T, right)
        return self.inv_cov_0 + num_y * quad

    def cov(self, y=None, **corr_kwargs):
        if y is None and not self._recompute_corr(**corr_kwargs):
            return self._cov
        return np.linalg.inv(self.inv_cov(y=y, **corr_kwargs))

    # @property
    # def inv_cov(self):
    #     return self._inv_cov

    # @property
    # def R(self):
    #     return self._R

    # @R.setter
    # def R(self, value):
    #     """Setter that ensures parameters remain in sync"""
    #     self._R = value
    #     self.R_chol = np.linalg.cholesky(self.R)
    #     # Must be in the following order:
    #     self._update_cov()
    #     self._update_means()
    #     self._update_scale()

    def chol(self, **corr_kwargs):
        if self._recompute_corr(**corr_kwargs):
            return np.linalg.cholesky(stabilize(self.corr(self.X, **corr_kwargs)))
        else:
            return self._chol

    @property
    def y(self):
        return self._y

    # @y.setter
    # def y(self, value):
    #     """Setter that ensures parameters remain in sync"""
    #     self._y = np.asarray(value)
    #     self.avg_y = np.average(self.y, axis=0)
    #     if (self.num_y, self.N) != self.y.shape:
    #         raise ValueError('y must remain the same shape')
    #     # Must be in the following order:
    #     self._update_shape()
    #     self._update_means()
    #     self._update_scale()

    @property
    def X(self):
        return self._X

    # @property
    # def H(self):
    #     return self._H

    def t_params(self, X=None, H=None, R=None, y=None, **corr_kwargs):
        if not self._recompute_corr(**corr_kwargs):
            corr_kwargs = self._corr_kwargs
        if X is None:
            X = self.X
        if H is None:
            H = self.basis(X)
        if R is None:
            R = self.corr(X, **corr_kwargs)
        shape = self.shape(y=y)
        scale = self.scale(y=y, **corr_kwargs)
        means = self.means(y=y, **corr_kwargs)
        cov = self.cov(y=y, **corr_kwargs)

        mean = np.dot(H, means)
        sigma = scale / shape * (R + np.dot(H, np.dot(cov, H.T)))
        df = 2 * shape
        return df, mean, sigma

    def _build_conditional(self, index, Xnew, corr=False, y=None, **corr_kwargs):
        # Set up variables
        if not self._recompute_corr(**corr_kwargs):
            corr_kwargs = self._corr_kwargs
        R_12 = self.corr(X=self.X, Xp=Xnew, **corr_kwargs)
        R_21 = R_12.T
        R_22 = self.corr(X=Xnew, Xp=None, **corr_kwargs)
        # H = self.H
        H = self.basis(self.X)
        R_chol = self.chol(**corr_kwargs)
        H_2 = self.basis(Xnew)

        # Compute conditional covariance
        Rinv_R12 = sp.linalg.cho_solve((R_chol, True), R_12)
        quad = np.dot(R_21, Rinv_R12)
        R_new = R_22 - quad
        if corr_scale is not None:
            R_new = corr_scale * R_new

        # Conditional basis
        Rinv_H = sp.linalg.cho_solve((R_chol, True), H)
        H_new = H_2 - np.dot(R_21, Rinv_H)
        if mean_scale is not None:
            H_new = mean_scale * H_new

        # Compute conditional parameters of t distribution
        df, mean, sigma = self.t_params(H=H_new, R=R_new, y=y, **corr_kwargs)
        y = y if y is not None else self.y
        Rinv_yn = sp.linalg.cho_solve((R_chol, True), y[index])
        mean_correction = np.dot(R_21, Rinv_yn)
        if mean_scale is not None:
            mean_correction = mean_scale.ravel() * mean_correction
        mean += mean_correction

        return df, mean, sigma

    def conditional(self, index, Xnew, corr=False, y=None, **corr_kwargs):
        df, mean, sigma = self._build_conditional(
            index, Xnew, corr=corr, y=y, **corr_kwargs)
        if corr:
            return MVT(mean=mean, sigma=sigma, df=df)
        else:
            scale = np.sqrt(np.diag(sigma))
            return st.t(df=df, loc=mean, scale=scale)

    def condition(self, index, Xnew, dob=None, y=None, **corr_kwargs):
        dist = self.conditional(index, Xnew, corr=False, y=y, **corr_kwargs)
        return predictions(dist, dob=dob)

    def predictive(self, X, corr=False, y=None, **corr_kwargs):
        """Returns a posterior predictive distribution object"""
        df, mean, sigma = self.t_params(X=X, y=y, **corr_kwargs)
        if corr:
            return MVT(mean=mean, sigma=sigma, df=df)
        else:
            scale = np.sqrt(np.diag(sigma))
            return st.t(df=df, loc=mean, scale=scale)

    def predict(self, X, dob=None, y=None, **corr_kwargs):
        dist = self.predictive(X, corr=False, y=y, **corr_kwargs)
        return predictions(dist, dob=dob)

    def evidence(self, log=True, y=None, **corr_kwargs):
        shape = self.shape(y=y)
        scale = self.scale(y=y, **corr_kwargs)
        means = self.means(y=y, **corr_kwargs)
        inv_cov = self.inv_cov(y=y, **corr_kwargs)
        cov = self.cov(y=y, **corr_kwargs)
        R_chol = self.chol(**corr_kwargs)
        y = y if y is not None else self.y
        num_y, N = y.shape

        tr_log_R = 2 * np.sum(np.log(np.diag(R_chol)))
        _, logdet_cov = np.linalg.slogdet(cov)

        ev = - 0.5 * num_y * (N * np.log(2*np.pi) + tr_log_R)
        ev += sp.special.gammaln(shape) + 0.5 * logdet_cov - \
            shape * np.log(scale)
        if self.inv_cov_0.any() and self.scale_0 != 0:  # If non-zero
            _, logdet_inv_cov_0 = np.linalg.slogdet(self.inv_cov_0)
            ev += - sp.special.gammaln(self.shape_0) + \
                0.5 * logdet_inv_cov_0 + self.shape_0 * np.log(self.scale_0)

        if not log:
            ev = np.exp(ev)
        return ev

    def posterior(self, name, logprior=None, log=False, **corr_kwargs):
        def ev(val):
            kw = {name: np.squeeze(val)}
            # print(val, np.squeeze(val))
            # print(corr_kwargs)
            return self.evidence(log=True, **kw, **corr_kwargs)

        vals = corr_kwargs.pop(name)
        log_pdf = np.apply_along_axis(ev, 1, np.atleast_2d(vals).T)
        if logprior is not None:
            log_pdf = log_pdf + logprior(**corr_kwargs)

        if not log:
            log_pdf -= np.max(log_pdf)
            pdf = np.exp(log_pdf)
            # Integrate using trapezoid rule
            norm = np.trapz(pdf, vals)
            return pdf/norm
        return log_pdf

    def corr_post(self, logprior=None, **corr_kwargs):
        """Evaluates the posterior for the correlation parameters in corr_kwargs

        Parameters
        ----------
        logprior : callable
        corr_kwargs : dict
            The values of the correlation parameters at which to evaluate the
            posterior. Because the evidence is vectorized, standard
            array broadcasting rules apply
        """
        # if not callable(logprior):
        #     raise ValueError('logprior must be callable')

        vec_evidence = np.vectorize(self.evidence)
        log_post = vec_evidence(log=True, **corr_kwargs)
        if logprior is not None:
            log_post = log_post + logprior(**corr_kwargs)
        log_post -= np.max(log_post)
        return np.exp(log_post)


class GeoProcess(MvBLM):

    # @property
    # def orders(self):
    #     return self._orders

    # @orders.setter
    # def orders(self, value):
    #     value = np.asarray(value)
    #     if value.ndim != 1:
    #         raise ValueError('orders must be a 1d array')
    #     self._orders = value
    #     self.ordersvec = np.atleast_2d(self.orders).T

    # @property
    # def ratio(self):
    #     return self._ratio

    # @ratio.setter
    # def ratio(self, value):
    #     # First rescale coefficients
    #     if hasattr(self, 'ratio') and hasattr(self, 'y'):
    #         old_ratio = self.ratio
    #         self.y = self.y * (old_ratio/value)**self.ordervec
    #     self._ratio = value

    def _recompute_coeffs(self, **ratio_kwargs):
        if ratio_kwargs and ratio_kwargs != self._ratio_kwargs:
            print('recomputing...')
            coeffs, orders = coefficients(
                partials=self.partials, ratio=self.ratio, X=self.X,
                ref=self.ref(self.X), orders=self._full_orders,
                rm_orders=self.rm_orders, **ratio_kwargs)
            return coeffs
        return None

    def _scale_rv(self, scale):
        scale_vec = np.atleast_2d(scale).T
        scale_cols, scale_rows = np.meshgrid(scale, scale)
        scale_mat = scale_cols * scale_rows
        return scale_vec, scale_mat

    def _geom_sum(self, r, a=1, start=0, end=None):
        if end < start:
            raise ValueError('start index of geometric sum cannot exceed end')
        return a * r**start * (1 - r**(end-start+1)) / (1 - r)

    def _ratio_matrix_sums(self, order, X=None, **ratio_kwargs):
        k = self.max_order
        X = X if X is not None else self.X
        ratio = self.ratio(X, **ratio_kwargs)
        ratio_vec, ratio_mat = self._scale_rv(ratio)
        mu_sum = self._geom_sum(r=ratio_vec, start=k+1, end=order)
        corr_sum = self._geom_sum(r=ratio_mat, start=k+1, end=order)
        return mu_sum, corr_sum

    def observe(self, X, partials, ratio, ref=1, orders=None,
                rm_orders=None, ratio_kwargs=None, **corr_kwargs):
        # if corr_kwargs is None:
        #     corr_kwargs = {}
        if ratio_kwargs is None:
            ratio_kwargs = {}

        self._ratio_kwargs = ratio_kwargs
        self._full_orders = orders

        self.partials = partials
        self.ratio = self._domain_function(ratio)
        self.ref = self._domain_function(ref)

        coeffs, orders = coefficients(
            partials=partials, ratio=ratio, X=X, ref=ref(X), orders=orders,
            rm_orders=rm_orders, **ratio_kwargs)
        self.orders = orders
        self.ordersvec = np.atleast_2d(self.orders).T
        self.rm_orders = rm_orders

        # Get max order
        max_order_arg = np.argmax(self.orders)
        self.max_order = orders[max_order_arg]
        self.max_partial = partials[max_order_arg]
        super(GeoProcess, self).observe(X=X, y=coeffs, **corr_kwargs)

    def _build_conditional(self, order, Xnew, corr=False, y=None,
                           rescale=False, predictive=False, max_order=None,
                           ratio_kwargs=None, **corr_kwargs):
        # Set up variables
        if ratio_kwargs is None:
            ratio_kwargs = self._ratio_kwargs
        if not self._recompute_corr(**corr_kwargs):
            corr_kwargs = self._corr_kwargs
        R_12 = self.corr(X=self.X, Xp=Xnew, **corr_kwargs)
        R_21 = R_12.T
        R_22 = self.corr(X=Xnew, Xp=None, **corr_kwargs)
        H = self.basis(self.X)
        R_chol = self.chol(**corr_kwargs)
        H_2 = self.basis(Xnew)

        # Compute conditional covariance
        Rinv_R12 = sp.linalg.cho_solve((R_chol, True), R_12)
        quad = np.dot(R_21, Rinv_R12)
        R_new = R_22 - quad

        # Conditional basis
        Rinv_H = sp.linalg.cho_solve((R_chol, True), H)
        H_new = H_2 - np.dot(R_21, Rinv_H)

        mean_sc = None
        if rescale:
            orders = [i for i in self._full_orders if i <= order]
            mean_sc, corr_sc = self._scale_rv(self.ratio(Xnew, **ratio_kwargs))
            mean_ref, corr_ref = self._scale_rv(self.ref(Xnew))
            mean_scale_lower = 0
            corr_scale_lower = 0
            for i in orders:
                mean_scale_lower += mean_sc**i
                corr_scale_lower += corr_sc**i
            R_new *= corr_ref * corr_scale_lower
            H_new *= mean_ref * mean_scale_lower
        else:
            orders = np.atleast_1d(order)

        indices = np.array([i for i in range(len(self._full_orders))])
        indices = indices[np.isin(self._full_orders, orders)]

        if predictive:
            H_pred, R_pred = self._build_predictive(
                Xnew, max_order=max_order, rescale=True,
                ratio_kwargs=ratio_kwargs, **corr_kwargs)
            R_new += R_pred
            H_new += H_pred

        # Compute conditional parameters of t distribution
        df, mean, sigma = self.t_params(H=H_new, R=R_new, y=y, **corr_kwargs)

        # y = y if y is not None else self.y
        coeffs, _ = coefficients(
                partials=self.partials, ratio=self.ratio, X=self.X,
                ref=self.ref(self.X), orders=self._full_orders,
                rm_orders=None, **ratio_kwargs)
        Rinv_yn = sp.linalg.cho_solve((R_chol, True), coeffs[indices].T)
        mean_correction = np.dot(R_21, Rinv_yn)
        if mean_sc is not None:
            mean_correction = np.sum(mean_ref * mean_sc**orders * mean_correction, axis=1)
        else:
            mean_correction = mean_correction.ravel()
        mean += mean_correction

        return df, mean, sigma

    def conditional(self, Xnew, order, corr=False, rescale=True,
                    ratio_kwargs=None, **corr_kwargs):
        if ratio_kwargs is None:
            ratio_kwargs = self._ratio_kwargs
        coeffs = self._recompute_coeffs(**ratio_kwargs)

        index = np.squeeze(np.argwhere(self.orders == order))
        df, mean, sigma = self._build_conditional(
            order=order, Xnew=Xnew, corr=corr, y=coeffs, rescale=rescale,
            **corr_kwargs)

        if corr:
            return MVT(mean=mean, sigma=sigma, df=df)
        else:
            scale = np.sqrt(np.diag(sigma))
            return st.t(df=df, loc=mean, scale=scale)

    def condition(self, Xnew, order, dob=None, rescale=True,
                  ratio_kwargs=None, **corr_kwargs):
        dist = self.conditional(Xnew=Xnew, order=order, rescale=rescale,
                                ratio_kwargs=ratio_kwargs, **corr_kwargs)
        return predictions(dist, dob=dob)

    def _build_predictive(self, X, max_order=None, rescale=True, ratio_kwargs=None, **corr_kwargs):
        H = self.basis(X)
        R = self.corr(X, **corr_kwargs)
        if rescale:
            # Sum contributions from geometric sum of ratios
            if max_order is None:
                max_order = np.inf
            ratio_mu, ratio_corr = \
                self._ratio_matrix_sums(max_order, X=X, **ratio_kwargs)
            # Also the overall multiplicative scale
            ref = self.ref(X)
            ref_vec, ref_mat = self._scale_rv(ref)
            # Scale basis and correlation
            H = ref_vec * ratio_mu * H
            R = ref_mat * ratio_corr * R
        return H, R

    def predictive(self, X, corr=False, max_order=None, rescale=True,
                   ratio_kwargs=None, **corr_kwargs):
        """Returns a posterior predictive distribution object"""
        if ratio_kwargs is None:
            ratio_kwargs = self._ratio_kwargs
        coeffs = self._recompute_coeffs(**ratio_kwargs)

        if rescale:
            df, mean, sigma = self._build_conditional(
                order=self.max_order, Xnew=X, corr=corr, y=coeffs,
                rescale=True, predictive=True, max_order=max_order,
                ratio_kwargs=ratio_kwargs, **corr_kwargs)
        else:
            H, R = self._build_predictive(
                X=X, max_order=max_order, rescale=rescale,
                ratio_kwargs=ratio_kwargs, **corr_kwargs)
            # Integrate out means and variance
            df, mean, sigma = self.t_params(X=X, H=H, R=R, y=coeffs,
                                            **corr_kwargs)

        if corr:
            return MVT(mean=mean, sigma=sigma, df=df)
        else:
            scale = np.sqrt(np.diag(sigma))
            return st.t(df=df, loc=mean, scale=scale)

    def predict(self, X, dob=None, max_order=None, rescale=True,
                ratio_kwargs=None, **corr_kwargs):
        dist = self.predictive(
            X, corr=False, max_order=max_order, rescale=rescale,
            ratio_kwargs=ratio_kwargs, **corr_kwargs)
        return predictions(dist, dob=dob)

    def ratio_post(self, ratio_kwargs, logprior=None, **corr_kwargs):
        if not callable(logprior):
            raise ValueError('logprior must be callable')

        def ratio_ev(**kwargs):
            coeffs = self._recompute_coeffs(**kwargs)
            return self.evidence(log=True, y=coeffs, **corr_kwargs)

        vec_evidence = np.vectorize(self.ratio_ev)
        log_post = vec_evidence(**ratio_kwargs)
        if logprior is not None:
            log_post = log_post + logprior(**ratio_kwargs)
        log_post -= np.max(log_post)
        return np.exp(log_post)


class GeoSeries(object):

    def __init__(self, shape=None, scale=None):
        if shape is None:
            self.shape_0 = 0
        else:
            self.shape_0 = shape

        if scale is None:
            self.scale_0 = 0
        else:
            self.scale_0 = scale

        self._shape = self.shape_0
        self._scale = self.scale_0

    def observe(self, partials, ratio, ref=1, orders=None,
                rm_orders=None, X=None, combine=False, **ratio_kwargs):

        self._ratio_kwargs = ratio_kwargs
        self._full_orders = orders

        self.partials = partials
        # self.ratio = self._domain_function(ratio)
        # self.ref = self._domain_function(ref)
        if callable(ratio):
            self.ratio = ratio(X, **ratio_kwargs)
        else:
            self.ratio = ratio

        if callable(ref):
            self.ref = ref(X)
        else:
            self.ref = ref

        coeffs, orders = coefficients(
            partials=partials, ratio=ratio, X=X, ref=ref, orders=orders,
            rm_orders=rm_orders, **ratio_kwargs)
        self.orders = orders
        self.ordersvec = np.atleast_2d(self.orders).T
        self.rm_orders = rm_orders

        # Get max order
        max_order_arg = np.argmax(self.orders)
        self.max_order = orders[max_order_arg]
        self.max_partial = partials[max_order_arg]
        self.coeffs = coeffs
        self.combine = combine
        self._shape = self.shape()

    def _recompute_coeffs(self, **ratio_kwargs):
        if ratio_kwargs and ratio_kwargs != self._ratio_kwargs:
            print('recomputing...')
            coeffs, orders = coefficients(
                partials=self.partials, ratio=self.ratio, X=self.X,
                ref=self.ref, orders=self._full_orders,
                rm_orders=self.rm_orders, **ratio_kwargs)
            return coeffs
        return self.coeffs

    def shape(self, **ratio_kwargs):
        coeffs = self._recompute_coeffs(**ratio_kwargs)
        num_c = coeffs.shape[0]
        return self.shape_0 + num_c / 2.0

    def scale(self, combine=False, **ratio_kwargs):
        coeffs = self._recompute_coeffs(**ratio_kwargs)
        csq = np.sum(coeffs**2, axis=0, dtype=float)
        return self.scale_0 + csq/2.0

    def predictive(self, order=None, rescale=True, **ratio_kwargs):
        shape = self.shape(**ratio_kwargs)
        scale = self.scale(**ratio_kwargs)
        df = 2 * shape
        mu = 0
        sd = scale / shape
        if rescale:
            if order is None:
                order = np.inf
            k = self.max_partial
            r = self.ratio
            summed_ratio = r**(k+1) * (1 - r**(order-k)) / (1 - r)
            sd *= summed_ratio * self.ref
            mu = self.max_partial
        return st.t(df=df, loc=mu, scale=sd)

    def predict(self, dob=None, order=None, rescale=True, **ratio_kwargs):
        dist = self.predictive(order=order, rescale=rescale, **ratio_kwargs)
        return predictions(dist, dob=dob)

    def evidence(self, log=True, combine=False, **ratio_kwargs):
        shape = self.shape(**ratio_kwargs)
        scale = self.scale(**ratio_kwargs)
        coeffs = self._recompute_coeffs(**ratio_kwargs)
        num_c = coeffs.shape[0]
        N = 1
        if combine:
            N = coeffs.shape[1]

        ev = - 0.5 * num_c * N * np.log(2*np.pi)
        ev += sp.special.gammaln(shape) - shape * np.log(scale)
        if self.scale_0 != 0:
            shape_0 = self.shape_0
            scale_0 = self.scale_0
            ev += - sp.special.gammaln(shape_0) + shape_0 * np.log(scale_0)

        if combine:
            ev = np.sum(ev, axis=0, dtype=float)
        if not log:
            ev = np.exp(ev)
        return ev

    def posterior(self, name, logprior=None, log=False, **ratio_kwargs):
        def ev(val):
            kw = {name: np.squeeze(val)}
            # print(val, np.squeeze(val))
            print(corr_kwargs)
            return self.evidence(log=True, combine=True, **kw, **ratio_kwargs)

        vals = ratio_kwargs.pop(name)
        log_pdf = np.apply_along_axis(ev, 1, np.atleast_2d(vals).T)
        if logprior is not None:
            log_pdf = log_pdf + logprior(**ratio_kwargs)

        if not log:
            log_pdf -= np.max(log_pdf)
            pdf = np.exp(log_pdf)
            # Integrate using trapezoid rule
            norm = np.trapz(pdf, vals)
            return pdf/norm
        return log_pdf


class GPCoeffs(pm.Model, CoeffBase):
    """Treats observables as sums of weighted iid Gaussian processes.

    Parameters for the Gaussian process are conditioned on observable data to
    permit the extimation of the truncation error.

    Parameters
    ----------
    name : str
        The name of the observable. This name will be
        placed before all RV names defined within this model,
        i.e. 'name_sd'.
    model : pymc3.Model object
        The parent model. If defined within a model context,
        it will use that one.
    """

    def __init__(self, X, obs, orders, rm_orders=None, ref=1, Q_est=1,
                 grid=False, name='', model=None, build=True,
                 **param_kwargs):
        # Can't use super since pm.Model doesn't accept kwargs
        # super(GPCoeffs, self).__init__(name=name, model=model)
        pm.Model.__init__(self, name=name, model=model)
        CoeffBase.__init__(
            self, X=X, obs=obs, orders=orders, rm_orders=rm_orders,
            ref=ref, Q_est=Q_est, grid=grid)
        if build:
            self.def_params(**param_kwargs)
            self.gp_model()

    def get_RV(self, rv):
        """Check both self and parent model for rv"""
        try:
            var = getattr(self, rv)
        except AttributeError:
            var = getattr(self.root, rv)
        return var

    def def_params(self, mu=0.0, sd=1.0, ls=1.0, sigma=1e-5, q=1.0):
        # Add a shape key to ls dict for anisotropic models
        if isinstance(ls, dict) and 'shape' not in ls:
            ls_size = 0
            try:
                ls_size = ls['mu'].shape[0]
            except (AttributeError, KeyError):
                pass
            try:
                sd_size = ls['sd'].shape[0]
            except (AttributeError, KeyError):
                pass
            else:
                if sd_size > ls_size:
                    ls_size = sd_size
            if ls_size > 1:
                ls['shape'] = ls_size

        # Convert non-dicts to dicts holding observed value
        args = [mu, sd, ls, sigma, q]
        for i, arg in enumerate(args):
            if not isinstance(arg, dict):
                args[i] = {'observed': arg}
        [mu, sd, ls, sigma, q] = args

        # Entering self ensures RVs are in the context of this instance
        # even if this method is called manually
        with self:
            pm.Normal('mu', **mu)
            pm.Lognormal('sd', **sd)
            pm.Lognormal('ls', **ls)
            pm.HalfNormal('sigma', **sigma)
            q = pm.Lognormal('q', **q)
            pm.Deterministic('Q', q * self.Q_est)
        return self

    def gp_model(self):
        with self:
            # Get predefined parameters
            mu = self.get_RV('mu')
            sd = self.get_RV('sd')
            ls = self.get_RV('ls')
            sigma = self.get_RV('sigma')
            try:
                q = self.get_RV('q')
            except AttributeError:  # If q hasn't been defined
                q = 1.0
                self.q = q

            # Define Q if it has been overlooked
            try:
                pm.Deterministic('Q', q * self.Q_est)
            except AttributeError:  # If q isn't an RV
                self.Q = tt.as_tensor_variable(q * self.Q_est)
            except ValueError:  # If Q is already defined
                pass

            # Setup mean and covariance functions
            orders = self.orders
            mean = pm.gp.mean.Constant(mu) * Exponential(b=q)
            B = tt.nlinalg.diag(q**(2*orders))
            coregion = pm.gp.cov.Coregion(1, B=B)
            covs = [coregion]
            if self.grid:  # Create cov for each grid entry
                Xgp = [orders[:, None], *self.Xs]
                num_Xs = len(self.Xs)
                for i, X in enumerate(self.Xs):
                    dim = X.shape[1]
                    try:
                        ls_i = ls[i]
                    except IndexError:  # Smaller length than Xs
                        print("Anisotropic models must have a distinct",
                              "length scale for each entry in Xs")
                        raise
                    except (TypeError, ValueError):  # Is scalar-like
                        ls_i = ls
                    cov = sd**(2.0/num_Xs) * pm.gp.cov.ExpQuad(dim, ls=ls_i)
                    covs.append(cov)
                ccov = pm.gp.cov.Kron(covs[1:])
            else:  # Create one cov
                Xgp = [orders[:, None], self.X]
                dim = self.X.shape[1]
                ccov = sd**2 * pm.gp.cov.ExpQuad(dim, ls=ls)
                covs.append(ccov)

            # Make Gaussian process and condition on data
            y = self.cs.ravel()
            gp = pm.gp.MarginalKron(mean_func=mean, cov_funcs=covs)
            gp.marginal_likelihood('cs_observed', Xs=Xgp, y=y, sigma=sigma)

        # For user access
        self.mean = mean
        self.covs = covs
        self.ccov = ccov
        self.gp = gp
        return self

    def setup_Deltak(self):
        # Get relevant variables
        k = self.orders[-1]
        mu = self.get_RV('mu')
        sd = self.get_RV('sd')
        ls = self.get_RV('ls')
        sigma = self.get_RV('sigma')
        Q = self.get_RV('Q')
        Q_est = self.Q_est

        # Set up variance matrix due to Q array
        rows, cols = tt.mgrid[0:self.N, 0:self.N]
        Qr, Qc = Q[rows], Q[cols]
        varQ = (Qr * Qc)**(k+1) / (1 - Qr * Qc)

        # Define variables for the Deltak process
        Dk_mean = pm.gp.mean.Constant(mu * Q**(k+1) / (1-Q))
        Dk_cov = varQ * self.ccov
        Dk_sigma = sigma * Q_est**(k+1) / tt.sqrt(1-Q_est**2)

        with self:  # Ensure Deltak belongs to this model context
            # In general, Q variance removes any potential Kronecker structure
            Dk_gp = pm.gp.Marginal(mean_func=Dk_mean, cov_func=Dk_cov)
            Dk = Dk_gp.marginal_likelihood(
                'Dk', X=self.X, y=None, noise=Dk_sigma, is_observed=False)
        return self


class InCoeffs(CoeffBase):
    """The observables are treated as sums of weighted Gaussian random variables.

    [description]
    """

    def __init__(self, X, obs, orders, rm_orders=None, ref=1, Q_est=1,
                 grid=False):
        super(InCoeffs, self).__init__(
            X=X, obs=obs, orders=orders, rm_orders=rm_orders, ref=ref,
            Q_est=Q_est, grid=grid)
        self.cksq = np.sum(self.cs**2, axis=0, dtype=float)
        self.qsq = self.Q_est**(2*self.k + 2) / (1.0 - self.Q_est**2)
        self.num_c = len(self.cs)

    def _a_n(self, a):
        return a + self.num_c/2.0

    def _b_n(self, b, data=None):
        if data is None:
            data = self.cksq
        return b + data/2.0

    def var_dist(self, a, b):
        return st.invgamma(a=self._a_n(a), scale=self._b_n(b))

    def error_dist(self, a, b, rescale=True):
        qsq = self.qsq
        a_n, b_n = self._a_n(a), self._b_n(b)
        loc = 0
        scale = np.sqrt(b_n * qsq / a_n)
        if rescale:
            loc = self.obsk
            scale *= self.ref
        return st.t(df=2*a_n, loc=loc, scale=scale)

    def error_pdf(self, Deltak, a, b, rescale=True):
        Dk = np.atleast_2d(Deltak).T
        dist = self.error_dist(a, b, rescale=rescale)
        return dist.pdf(Dk).T

    def error_interval(self, alpha, a, b, rescale=True):
        """Returns the centered degree of belief interval for Delta_k

        Parameters
        ----------
        alpha : float or ndarray
            The specifies the 100*alpha% interval
        a : float
            The hyperparameter of the variance prior `InvGam(a, b)`
        b : float
            The hyperparameter of the variance prior `InvGam(a, b)`
        rescale : bool, optional
            Whether or not the dimensionless error is scaled relative to the
            highest order observable obs_k, that is, obs_k + ref * error, the
            default is True

        Returns
        -------
        tuple
            The lower and upper bounds of the error interval
        """
        dist = self.error_dist(a, b, rescale=rescale)
        low, up = dist.interval(alpha=alpha)
        # if rescale:
        #     obsk, ref = self.obsk, self.ref
        #     low, up = obsk + ref*low, obsk + ref*up
        return low, up

    def fQ_prior_logpdf(self, fQ, a_fQ, b_fQ, scale=1.0, inverted=False):
        # if inverted:
        #     prior = st.invgamma(a=a_fQ, scale=b_fQ)
        # else:
        #     # Use rate param for consistency: b=1/scale
        #     scale = 1.0/b_fQ
        #     prior = st.gamma(a=a_fQ, scale=scale)
        # return prior
        if inverted:
            logpdf = st.beta.logpdf(1/fQ, a=a_fQ, b=b_fQ, scale=1/scale)
            logpdf -= 2*np.log(fQ)
        else:
            logpdf = st.beta.logpdf(fQ, a=a_fQ, b=b_fQ, scale=scale)
        return logpdf

    def fQ_ulogpdf(self, fQ, a, b, a_fQ, b_fQ, scale=1.0, inverted=False,
                   combine=True):
        """The unnormalized logpdf for the expansion parameter.

        See the docstring for expar_pdf.
        """
        # Handle scaling and inverting
        # x = x/scale
        # x = np.atleast_2d(x).T
        Q = fQ / scale
        if inverted:
            Q = Q**(-1)
        # if np.any(Q >= 1):
        #     raise ValueError('Q must be between 0 and 1')
        Q = np.atleast_2d(Q).T

        # Set up terms
        orders, cs, Q_est = self.orders, self.cs, self.Q_est
        cs_Q = np.array([cn*(Q_est/Q)**n for n, cn in zip(orders, cs)])
        cksq_Q = np.sum(cs_Q**2, axis=0)

        a_n = self._a_n(a)
        b_n = self._b_n(b, data=cksq_Q)
        logp = - a_n * np.log(b_n) - np.sum(orders) * np.log(Q)
        log_prior = self.fQ_prior_logpdf(fQ, a_fQ, b_fQ, scale, inverted)
        if combine:
            logp = np.sum(logp, axis=1)
            logp += log_prior
        else:
            logp += np.atleast_2d(log_prior).T
            logp = logp.T
        return logp

    def fQ_pdf(self, fQ, a, b, a_fQ, b_fQ, scale=1.0, inverted=False,
               combine=True):
        """The approximately normalized pdf for the expansion parameter.

        The pdf is normalized using the trapezoid rule based on the input
        points fQ. Hence fine grids that capture the probability mass will
        be accurately normalized.

        Parameters
        ----------
        fQ : ndarray
            A function f of the expansion parameter Q: f(Q)
        a : float
            [description]
        b : float
            [description]
        a_expar : float
            [description]
        b_expar : float
            [description]
        scale : float, optional
            [description] the default is 1.0
        inverted : bool, optional
            [description] (the default is False, which [default_description])
        combine : bool, optional
            Whether the obserables should use a common expansion parameter, the
            default is True

        Returns
        -------
        ndarray
            The pdf for f(Q)
        """
        logp = self.fQ_ulogpdf(fQ, a, b, a_fQ, b_fQ, scale=scale,
                               inverted=inverted, combine=combine)
        # Reduce underflow and overflow
        maxlogp = np.atleast_2d(np.max(logp, axis=-1)).T
        logp = logp - maxlogp
        pdf = np.exp(logp)

        # Integrate using trapezoid rule
        norm = np.atleast_2d(np.trapz(pdf, fQ)).T
        return np.squeeze(pdf/norm)

    def fQ_interval(self, alpha, fQ, a, b, a_fQ, b_fQ, scale=1.0,
                    inverted=False, combine=True):
        pdf = self.fQ_pdf(fQ, a, b, a_fQ, b_fQ, scale=scale,
                          inverted=inverted, combine=combine)
        cdf = integrate.cumtrapz(pdf, x=fQ, initial=0)

        # Invert cdf
        alpha = np.asarray(alpha)
        if np.any((alpha > 1) | (alpha < 0)):
            raise ValueError("alpha must be between 0 and 1 inclusive")
        q1, q2 = (1.0-alpha)/2, (1.0+alpha)/2
        low = (np.abs(cdf-q1)).argmin(axis=-1)
        up = (np.abs(cdf-q2)).argmin(axis=-1)
        return fQ[low], fQ[up]
